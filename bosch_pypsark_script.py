#%% Adatok gener√°l√°sa DataFrame-be

import os
import random
import pandas as pd
from datetime import datetime, timedelta


# Alapbe√°ll√≠t√°sok
# ==========================
N_ROWS = 1000000  # tesztel√©shez; k√©s≈ëbb 1000000-ra √°ll√≠tjuk
OUTPUT_DIR = "data/raw"
OUTPUT_FILE = "sensor_data_test.csv"

# Mappa l√©trehoz√°sa, ha nem l√©tezik
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Param√©ter tartom√°nyok
PARAMETER_RANGES = {
    "Temperature": (-20, 120),
    "Noise": (0, 150),
    "Offset": (-5, 5),
    "Vibration": (0, 10),
}

# Seg√©df√ºggv√©nyek
# ==========================
def random_sensor_id():
    """V√©letlenszer≈± 7 karakteres azonos√≠t√≥ bet≈±kb≈ël √©s sz√°mokb√≥l."""
    chars = "abcdefghijklmnopqrstuvwxyz0123456789"
    return "".join(random.choices(chars, k=7))

def random_date(start_date, end_date):
    """V√©letlen d√°tum k√©t id≈ëpont k√∂z√∂tt (nap pontoss√°ggal)."""
    delta = end_date - start_date
    random_days = random.randint(0, delta.days)
    return (start_date + timedelta(days=random_days)).strftime("%Y-%m-%d")

def generate_value(parameter, status):
    """Konzisztens √©rt√©k gener√°l√°sa a param√©ter √©s st√°tusz alapj√°n."""
    low, high = PARAMETER_RANGES[parameter]
    value = random.uniform(low, high)
    # Ha 'Bad' a st√°tusz, dobjunk be n√©ha outlier √©rt√©ket
    if status == "Bad":
        # pl. 10%-ban extr√©m √©rt√©k
        if random.random() < 0.1:
            value *= random.choice([-3, 3])
    return round(value, 2)

def generate_sensor_data(n_rows: int):
    locations = ["Hungary", "Germany", "UK", "China", "Mexico"]
    parameters = list(PARAMETER_RANGES.keys())
    statuses = ["Good", "Bad"]

    start_date = datetime(2025, 1, 1)
    end_date = datetime(2025, 10, 28)

    data = []
    for _ in range(n_rows):
        status = random.choices(statuses, weights=[0.9, 0.1])[0]
        parameter = random.choice(parameters)
        value = generate_value(parameter, status)

        row = {
            "Sensor_ID": random_sensor_id(),
            "Date": random_date(start_date, end_date),
            "Location": random.choice(locations),
            "Parameter": parameter,
            "Value": value,
            "Status": status,
        }
        data.append(row)

    return pd.DataFrame(data)

# ==========================
# F≈ë futtat√°s
# ==========================
if __name__ == "__main__":
    df = generate_sensor_data(N_ROWS)
    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)
    df.to_csv(output_path, index=False)
    print(f"{len(df)} sor gener√°lva -> {output_path}")
    print(df.head())
    
    
#%%  PySpark k√∂rnyezet inicializ√°l√°s , √©s adatbeolvas√°s

import os
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Dummy hadoop be√°ll√≠t√°s a hibe√ºzenet elker√ºl√©s√©re
os.environ["JAVA_HOME"] = r"C:\Program Files\Eclipse Adoptium\jdk-17.0.16.8-hotspot"
os.environ["HADOOP_HOME"] = r"C:\hadoop"
#os.makedirs("C:\\hadoop\\bin", exist_ok=True)

# Spark inicializ√°l√°s
# ==========================
spark = (
    SparkSession.builder
    .appName("BoschSparkTest")
    .master("local[*]")
    .getOrCreate()
)

print("SparkSession l√©trej√∂tt:", spark.version)


# Adatbeolvas√°s
# ==========================
input_path = "data/raw/sensor_data_test.csv"

df = (
    spark.read.option("header", True)
    .option("inferSchema", True)
    .csv(input_path)
)


df.printSchema()
df.show(5)

#%% SPark Transzform√°ci√≥k

# Alap transzform√°ci√≥k
# ==========================

# √Åtlagos √©rt√©k param√©teren √©s st√°tuszonk√©nt
agg_df = (
    df.groupBy("Parameter", "Status")
      .agg(
          F.count("*").alias("count_rows"),
          F.avg("Value").alias("avg_value"),
          F.stddev("Value").alias("stddev_value")
      )
      .orderBy("Parameter", "Status")
)

print("üìä Aggreg√°lt statisztika:")
agg_df.show()

# Window p√©ldak√©nt: hogy adott szenzoron bel√ºl hanyadik minta
window_spec = Window.partitionBy("Sensor_ID").orderBy("Date")
df_with_rank = df.withColumn("row_num", F.row_number().over(window_spec))
print("P√©lda Window f√ºggv√©nyre:")
df_with_rank.select("Sensor_ID", "Date", "Parameter", "Value", "row_num").show(5)

# Napi √°tlag param√©terenk√©nt
pivot_df = (
    df.groupBy("Date", "Location")
      .pivot("Parameter")
      .agg(F.avg("Value"))
      .orderBy("Date")
)

print("üìà Pivot DataFrame:")
pivot_df.show(5)

#%% 
# Ment√©s Parquet-be
# ==========================
output_dir = "data/processed"
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, "sensor_processed.parquet")

pivot_df.write.mode("overwrite").parquet(output_path)
print(f"mentve Parquet form√°tumban ide: {output_path}")


# Lez√°r√°s
# ==========================
spark.stop()
print("üßπ SparkSession le√°ll√≠tva.")